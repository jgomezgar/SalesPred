{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for evaluate a model trained.\n",
    "\n",
    "The same process of transformation of the test data is followed as was done for the training data for the correct functioning of the model prediction. The results will reflect the true effectiveness of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GroupKFold, cross_val_score\n",
    "import lightgbm as ltb\n",
    "import matplotlib.pyplot as plt \n",
    "import joblib\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From an fitted model makes a prediction and returns the results.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Model object fitted.\n",
    "        X: Data with the independent variables to predict\n",
    "        y: Data with the dependient variable to compare with the prediction\n",
    "        thresholds: List of thresholds where to check for each of them, the \n",
    "        number of records of X below them.\n",
    "        verbose: Defines whether or not the output is displayed.\n",
    "    \n",
    "    Returns:\n",
    "        results: Pandas Dataframe with the results of the predictions:\n",
    "            - REAL: Real value of dependent variable.\n",
    "            - PRED: Prediction value of dependent variable.\n",
    "            - PERCENTAGE_ERROR:  Percentage deviation mean error by row from REAL and PRED.\n",
    "            - ABSOLUTE_ERROR: Absolute mean error by row from REAL and PRED.\n",
    "            - R2_SCORE: R square score from REAL and PRED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X, y, thresholds=[5], verbose=1):\n",
    "    y_pred = model.predict(X)\n",
    "    y = np.power(y,3)\n",
    "    y_pred = np.power(y_pred,3)\n",
    "    porc_error = abs(y_pred - y)*100/y\n",
    "    absolute_mean_error = mean_absolute_error(y, y_pred)\n",
    "    porcentual_mean_error = np.mean(porc_error[porc_error != np.inf])\n",
    "    if verbose:\n",
    "        print(\"\\nTEST: Absolute Error:\", absolute_mean_error)\n",
    "        print(\"Porcentual Error:\", porcentual_mean_error)\n",
    "        print(\"STD Error:\", np.std(abs(y - y_pred)))\n",
    "        print(\"R2 Score:\", r2_score(y, y_pred)) #Coef determ\n",
    "    results = pd.DataFrame(np.array(y), columns = ['REAL'])\n",
    "    results['PRED'] = y_pred\n",
    "    results['PERCENTAGE_ERROR'] = np.abs(results['PRED'] - results['REAL'])*100/results['REAL'] \n",
    "    results['ABSOLUTE_ERROR'] = np.abs(results['PRED'] - results['REAL'])*100\n",
    "    results['R2_SCORE'] = r2_score(results['REAL'], results['PRED'])\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        hits = 0\n",
    "        for element in results['ABSOLUTE_ERROR']:\n",
    "            if element <= threshold:\n",
    "                hits+=1\n",
    "        \n",
    "        porcentual_hits = hits*100/len(results)\n",
    "        if verbose:\n",
    "            print(str(porcentual_hits)+str(\"% registers are with less than\"), str(threshold)+str(\"% of absolute error.\"))\n",
    "            results['% REG ERROR < '+str(threshold)] = porcentual_hits\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define INPUT and OUTPUT files\n",
    "\n",
    "INPUT = '../../F3.Data Preparation/02_Data/Direct_sampled_test.csv'\n",
    "INPUT_FEATS = '../../F4.Modeling/02_Data/Direct_AllFeat_features.npy'\n",
    "INPUT_MODEL = '../../F4.Modeling/02_Data/Direct_AllFeat_model.pkl'\n",
    "INPUT_SC = '../../F4.Modeling/02_Data/Direct_AllFeat_sc_X.bin'\n",
    "OUTPUT = '../02_Data/Direct_AllFeat_results_test.csv'\n",
    "OUTPUT_TEST_RES = '../02_Data/Direct_AllFeat_test_results.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data resetting the indexes\n",
    "data = pd.read_csv(INPUT, sep='|').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../F4.Modeling/02_Data/Direct_AllFeat_features.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b4e54079b488>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read best features from feature selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINPUT_FEATS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../F4.Modeling/02_Data/Direct_AllFeat_features.npy'"
     ]
    }
   ],
   "source": [
    "# Read best features from feature selection\n",
    "features = np.load(INPUT_FEATS).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing the independent and dependent variables for test data in X and y respectively\n",
    "\n",
    "X_test = data[features]\n",
    "y_test = data['QUOTA_SELLOUT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load StandardScaler used in the traning phase and transform the test data\n",
    "sc_X = joblib.load(INPUT_SC)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Gaussian transformation as the one used in training for dependent variable\n",
    "y_test = y_test**(float(1)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and evaluate with the test data\n",
    "model = joblib.load(INPUT_MODEL)\n",
    "res_test = eval_model(model, X_test, y_test, [1, 2, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format results dataframe for write to file\n",
    "res_test = data[['CUSTOMER_ID','BRANDFAMILY_ID','DATE_init','DATE_end']].reset_index(drop=True).merge(res_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is written to a file\n",
    "res_test.to_csv(OUTPUT_TEST_RES, sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time.time()\n",
    "print (\"Time to execute script:\",str(round((t2-t1)/3600,2)), \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
